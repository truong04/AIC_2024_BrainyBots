{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-22T08:32:09.967667Z",
     "start_time": "2024-09-22T08:32:09.965556Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import cv2\n",
    "from sklearn.cluster import DBSCAN\n",
    "from PIL import Image\n"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T08:32:12.177919Z",
     "start_time": "2024-09-22T08:32:10.011059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('clip-ViT-L-14')\n"
   ],
   "id": "459d5dfa51dd3d90",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daoan/Projects/AI_Challenge_HCMC_2024/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T08:32:12.184250Z",
     "start_time": "2024-09-22T08:32:12.181816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_frame_features(video_path):\n",
    "    \"\"\"\n",
    "    Extract features from each frame of the video.\n",
    "\n",
    "    Parameters:\n",
    "        video_path (str): Path to the input video.\n",
    "\n",
    "    Returns:\n",
    "        frame_indices (list): List of frame indices.\n",
    "        features (np.ndarray): Array of extracted features.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Cannot open video: \" + video_path)\n",
    "        return [], np.array([])\n",
    "\n",
    "    frame_indices = []\n",
    "    features = []\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the frame from BGR to RGB and then to PIL Image\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image_pil = Image.fromarray(frame_rgb)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Compute the embedding and normalize it\n",
    "            image_embedding = model.encode(\n",
    "                [image_pil],\n",
    "                convert_to_tensor=True,\n",
    "                show_progress_bar=False\n",
    "            )[0]\n",
    "            image_embedding = image_embedding / image_embedding.norm()\n",
    "\n",
    "            # Append the embedding and frame index\n",
    "            features.append(image_embedding.cpu().numpy())\n",
    "            frame_indices.append(frame_count)\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    return frame_indices, np.array(features)\n"
   ],
   "id": "fbfb6c6ba3ef5039",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T08:32:12.225854Z",
     "start_time": "2024-09-22T08:32:12.223639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "\n",
    "def cluster_frames(features, eps=0.3, min_samples=5):\n",
    "    # Compute cosine distance matrix\n",
    "    distance_matrix = cosine_distances(features)\n",
    "\n",
    "    # Apply DBSCAN clustering\n",
    "    dbscan = DBSCAN(metric='precomputed', eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(distance_matrix)\n",
    "\n",
    "    return labels"
   ],
   "id": "b16b7b61c994f24f",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T08:32:12.271428Z",
     "start_time": "2024-09-22T08:32:12.267525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_video_slots(video_path, frame_indices, labels, output_dir):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Cannot open video: \" + video_path)\n",
    "        return\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Group frame indices by cluster label\n",
    "    clusters = {}\n",
    "    for idx, label in zip(frame_indices, labels):\n",
    "        if label == -1:\n",
    "            continue\n",
    "        clusters.setdefault(label, []).append(idx)\n",
    "\n",
    "    # Sort frames within each cluster\n",
    "    for frames in clusters.values():\n",
    "        frames.sort()\n",
    "\n",
    "    # Get OpenCV version\n",
    "    opencv_version = cv2.__version__\n",
    "\n",
    "    # Extract video slots\n",
    "    for label, frames in clusters.items():\n",
    "        if not frames:\n",
    "            continue\n",
    "\n",
    "        start_frame = frames[0]\n",
    "        end_frame = frames[-1]\n",
    "\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        # Determine codec and output file extension\n",
    "        if int(opencv_version.split('.')[0]) < 3:\n",
    "            # For OpenCV 2.x\n",
    "            fourcc = cv2.cv.CV_FOURCC(*'XVID')\n",
    "            ext = 'avi'\n",
    "        else:\n",
    "            # For OpenCV 3.x and above\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "            ext = 'avi'\n",
    "\n",
    "        output_path = os.path.join(output_dir, f\"slot_{label}.{ext}\")\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "        for frame_num in range(start_frame, end_frame + 1):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            out.write(frame)\n",
    "\n",
    "        out.release()\n",
    "        print(f\"Extracted slot {label}: Frames {start_frame} to {end_frame}\")\n",
    "\n",
    "    cap.release()\n"
   ],
   "id": "12023e7f1c88c895",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T08:32:12.313745Z",
     "start_time": "2024-09-22T08:32:12.312275Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "47ecd4ed1ddbbe1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T08:35:30.926703Z",
     "start_time": "2024-09-22T08:32:12.356562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_video(video_path, output_dir, eps=0.5, min_samples=5):\n",
    "    \"\"\"\n",
    "    Process the video to extract features, cluster frames, and extract video slots.\n",
    "\n",
    "    Parameters:\n",
    "        video_path (str): Path to the input video.\n",
    "        output_dir (str): Directory to save the extracted video slots.\n",
    "        eps (float): DBSCAN eps parameter.\n",
    "        min_samples (int): DBSCAN min_samples parameter.\n",
    "    \"\"\"\n",
    "    # Step 1: Extract frame features\n",
    "    print(\"Extracting features from video frames...\")\n",
    "    frame_indices, features = extract_frame_features(video_path)\n",
    "\n",
    "    if features.size == 0:\n",
    "        print(\"No features extracted. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Cluster frames using DBSCAN\n",
    "    print(\"Clustering frames with DBSCAN...\")\n",
    "    labels = cluster_frames(features, eps=eps, min_samples=min_samples)\n",
    "\n",
    "    # Step 3: Extract video slots based on clustering\n",
    "    print(\"Extracting video slots based on clustering...\")\n",
    "    extract_video_slots(video_path, frame_indices, labels, output_dir)\n",
    "\n",
    "    print(\"Processing complete.\")\n",
    "\n",
    "\n",
    "# Path to the video\n",
    "video_path = '/media/daoan/T7 Shield2/AI_Challenge_2024_DATA/video_with_audio/Videos_L23/video/L23_V001.mp4'\n",
    "\n",
    "# Directory to save extracted video slots\n",
    "output_dir = '/home/daoan/Projects/AI_Challenge_HCMC_2024/data_extraction/frame_split/video'\n",
    "\n",
    "# Parameters for DBSCAN\n",
    "eps = 0.3  # Adjusted for cosine distance (range 0 to 2)\n",
    "min_samples = 5\n",
    "\n",
    "# Process the video\n",
    "process_video(video_path, output_dir, eps=eps, min_samples=min_samples)"
   ],
   "id": "29f6ee2f854639af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from video frames...\n",
      "Clustering frames with DBSCAN...\n",
      "Extracting video slots based on clustering...\n",
      "Extracted slot 0: Frames 0 to 4398\n",
      "Processing complete.\n"
     ]
    }
   ],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
